{"cells":[{"cell_type":"code","source":["\n","import os\n","import json\n","from datetime import datetime\n","\n","def load_or_initialize_metadata(metadata_file_path):\n","    if not os.path.exists(metadata_file_path):\n","        os.makedirs(os.path.dirname(metadata_file_path), exist_ok=True)\n","        with open(metadata_file_path, 'w') as meta_file:\n","            json.dump({}, meta_file)\n","        return {}\n","    try:\n","        with open(metadata_file_path, 'r') as meta_file:\n","            content = meta_file.read().strip()\n","            return json.loads(content) if content else {}\n","    except json.JSONDecodeError:\n","        print(\"‚ö†Ô∏è Metadata file is corrupted. Initializing new metadata.\")\n","        return {}\n","\n","def collect_sql_files(sql_root_path):\n","    sql_files = []\n","    required_schemas = set()\n","\n","    for root, dirs, files in os.walk(sql_root_path):\n","        # Sort directories to ensure correct traversal order (numeric-aware)\n","        dirs.sort(key=lambda d: int(d) if d.isdigit() else d)\n","\n","        # Sort files within each folder\n","        for f in sorted(files):\n","            if f.endswith(\".sql\"):\n","                full_path = os.path.join(root, f)\n","                modified_time = os.path.getmtime(full_path)\n","                modified_dt = datetime.fromtimestamp(modified_time)\n","                base_name = f[:-4]\n","                schema, table_name = base_name.split('.', 1) if '.' in base_name else (\"default\", base_name)\n","                required_schemas.add(schema)\n","                sql_files.append({\n","                    \"schema\": schema,\n","                    \"table_name\": table_name,\n","                    \"path\": full_path,\n","                    \"timestamp\": modified_time,\n","                    \"datetime\": modified_dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n","                })\n","\n","    return sql_files, required_schemas\n","\n","def ensure_schemas_exist(required_schemas, dry_run=False):\n","    existing_schemas = set(row.namespace.split('.')[-1] for row in spark.sql(\"SHOW SCHEMAS\").collect())\n","    for schema in required_schemas - existing_schemas:\n","        print(f\"{'Would create' if dry_run else 'Creating'} missing schema: {schema}\")\n","        if not dry_run:\n","            try:\n","                spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema}\")\n","                print(f\"‚úÖ Schema '{schema}' created.\")\n","            except Exception as e:\n","                print(f\"‚ùå Failed to create schema '{schema}': {e}\")\n","\n","def drop_obsolete_mlvs(sql_files, mlv_metadata, dry_run=False):\n","    all_schemas = set(row.namespace.split('.')[-1] for row in spark.sql(\"SHOW SCHEMAS\").collect())\n","    existing_mlvs = set()\n","\n","    for schema in all_schemas:\n","        try:\n","            result = spark.sql(f\"SHOW MATERIALIZED LAKE VIEWS IN {schema}\").collect()\n","            for row in result:\n","                existing_mlvs.add((schema, row.name))\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Could not list MLVs in schema '{schema}': {e}\")\n","\n","    # Convert sql_files list to set of tuples for comparison\n","    sql_file_keys = {(item['schema'], item['table_name']) for item in sql_files}\n","    mlvs_to_drop = existing_mlvs - sql_file_keys\n","\n","    for schema, mlv in mlvs_to_drop:\n","        print(f\"{'Would drop' if dry_run else 'Dropping'} obsolete MLV: {schema}.{mlv}\")\n","        if not dry_run:\n","            try:\n","                spark.sql(f\"DROP MATERIALIZED LAKE VIEW IF EXISTS {schema}.{mlv}\")\n","                print(f\"‚úÖ Dropped obsolete MLV: {schema}.{mlv}\")\n","                mlv_metadata.pop(f\"{schema}.{mlv}\", None)\n","            except Exception as e:\n","                print(f\"‚ùå Failed to drop MLV '{schema}.{mlv}': {e}\")\n","\n","    return existing_mlvs\n","\n","def create_or_update_mlvs(sql_files, existing_mlvs, mlv_metadata, dry_run=False):\n","    for file_info in sql_files:\n","        schema = file_info[\"schema\"]\n","        table_name = file_info[\"table_name\"]\n","        file_path = file_info[\"path\"]\n","        modified_datetime = datetime.strptime(file_info[\"datetime\"], \"%Y-%m-%d %H:%M:%S\")\n","        metadata_key = f\"{schema}.{table_name}\"\n","        last_processed_str = mlv_metadata.get(metadata_key, {}).get(\"datetime\", \"1970-01-01 00:00:00\")\n","        last_processed_datetime = datetime.strptime(last_processed_str, \"%Y-%m-%d %H:%M:%S\")\n","\n","        # Note: ensure '>' is a literal greater-than (avoid pasted HTML entities like &gt;)\n","        if modified_datetime > last_processed_datetime:\n","            with open(file_path, 'r') as file:\n","                select_statement = file.read().strip()\n","\n","            print(f\"{'Would create/replace' if dry_run else 'Creating or replacing'} MLV: {schema}.{table_name}\")\n","            if not dry_run:\n","                try:\n","                    # No DROP anymore; rely on CREATE OR REPLACE\n","                    create_sql = (\n","                        f\"CREATE OR REPLACE MATERIALIZED LAKE VIEW \"\n","                        f\"{schema}.{table_name} {select_statement}\"\n","                    )\n","                    spark.sql(create_sql)\n","                    print(f\"‚úÖ MLV '{schema}.{table_name}' created or replaced successfully.\")\n","\n","                    mlv_metadata[metadata_key] = {\n","                        \"timestamp\": file_info[\"timestamp\"],\n","                        \"datetime\": file_info[\"datetime\"]\n","                    }\n","                except Exception as e:\n","                    print(f\"‚ùå Failed to create/replace MLV '{schema}.{table_name}': {e}\")\n","\n","def save_metadata(metadata_file_path, mlv_metadata):\n","    with open(metadata_file_path, 'w') as meta_file:\n","        json.dump(mlv_metadata, meta_file, indent=2)\n","\n","# === Main Execution ===\n","def main(dry_run=False):\n","    sql_root_path = \"/lakehouse/default/Files/MLV\"\n","    metadata_file_path = os.path.join(sql_root_path, \"mlv_metadata.json\")\n","\n","    mlv_metadata = load_or_initialize_metadata(metadata_file_path)\n","\n","    sql_files, required_schemas = collect_sql_files(sql_root_path)\n","\n","    ensure_schemas_exist(required_schemas, dry_run)\n","\n","    existing_mlvs = drop_obsolete_mlvs(sql_files, mlv_metadata, dry_run)\n","\n","    create_or_update_mlvs(sql_files, existing_mlvs, mlv_metadata, dry_run)\n","\n","    if not dry_run:\n","        save_metadata(metadata_file_path, mlv_metadata)\n","    else:\n","        print(\"üìù Dry run mode: Metadata not saved.\")\n","\n","##################################################################################################\n","# Run\n","main(dry_run=False)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2cbf967e-1c6c-4d77-b298-5693733ae511"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"f4d69089-3d20-442a-84a1-4256c19b5787","known_lakehouses":[{"id":"f4d69089-3d20-442a-84a1-4256c19b5787"}],"default_lakehouse_name":"Datamonster_Backend_lh","default_lakehouse_workspace_id":"0a279fa4-85b1-4a26-8497-b10ff174350c"}}},"nbformat":4,"nbformat_minor":5}